{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d283db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Batch x: torch.Size([128, 1, 28, 28]) Batch y: torch.Size([128]) Labels sample: [1, 2, 8, 5, 2, 6, 9, 9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- Block 1: Imports + Dataset -----\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get always the same results\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# MNIST: pixels between 0 and 1\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Quick sanity check: visualize a few samples\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch x:\", x.shape, \"Batch y:\", y.shape, \"Labels sample:\", y[:8].tolist())\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(10, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(x[i, 0].numpy(), cmap=\"gray\")\n",
    "    ax.set_title(str(y[i].item()))\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3885da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    }
   ],
   "source": [
    "saved_input_idx = 0\n",
    "saved_input_sample = x[saved_input_idx:saved_input_idx+1].to(device)\n",
    "saved_input_label  = y[saved_input_idx:saved_input_idx+1].to(device)\n",
    "plt.imshow(saved_input_sample[0, 0].cpu().numpy(), cmap=\"gray\")\n",
    "plt.title(f\"Label: {saved_input_label.item()}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Export input in a vector between 0 and 1 in a npz\n",
    "print(saved_input_sample.cpu().view(saved_input_sample.size(0), -1).numpy().shape)\n",
    "np.savez_compressed(\"mnist_sample_input.npz\", input=saved_input_sample.cpu().view(saved_input_sample.size(0), -1).numpy(), label=saved_input_label.cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baabf448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7628/3182847473.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | test_loss=0.6788 | test_acc=62.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7628/3182847473.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | test_loss=0.0415 | test_acc=98.97%\n"
     ]
    }
   ],
   "source": [
    "# ----- Block 2: One neuron Is it a 0 ?  -----\n",
    "\n",
    "# Create a new loader that map label (1 if 0 else 0)\n",
    "def make_binary_loaders(batch_size=128):\n",
    "    binary_target = lambda t: 1 if int(t) == 0 else 0\n",
    "\n",
    "    train_bin = datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform, target_transform=binary_target\n",
    "    )\n",
    "    test_bin = datasets.MNIST(\n",
    "        root=\"./data\", train=False, download=True, transform=transform, target_transform=binary_target\n",
    "    )\n",
    "\n",
    "    train_bin_loader = DataLoader(train_bin, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_bin_loader  = DataLoader(test_bin,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_bin_loader, test_bin_loader\n",
    "\n",
    "train_bin_loader, test_bin_loader = make_binary_loaders(BATCH_SIZE)\n",
    "\n",
    "class SingleNeuronBinary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 1)  # 784 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)      # flatten\n",
    "        out = self.fc(x)            \n",
    "        return out\n",
    "\n",
    "model_bin = SingleNeuronBinary().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()     # sigmoid included internally\n",
    "optimizer = torch.optim.SGD(model_bin.parameters(), lr=0.1)\n",
    "\n",
    "def eval_binary(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = torch.tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "            preds = (out >= 0).float()  # threshold at 0 logit\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "# Training loop\n",
    "test_loss, test_acc = eval_binary(model_bin, test_bin_loader)\n",
    "print(f\"Epoch 0 | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model_bin.train()\n",
    "    for x, y in train_bin_loader:\n",
    "        x = x.to(device)\n",
    "        y = torch.tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_bin(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_loss, test_acc = eval_binary(model_bin, test_bin_loader)\n",
    "    print(f\"Epoch {epoch:02d} | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Export weights for later VHDL conversion (float)\n",
    "W_bin = model_bin.fc.weight.detach().cpu().numpy()  # shape (1,784)\n",
    "b_bin = model_bin.fc.bias.detach().cpu().numpy()    # shape (1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986eb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | test_loss=2.3419 | test_acc=9.04%\n",
      "Epoch 01 | test_loss=0.3812 | test_acc=90.03%\n"
     ]
    }
   ],
   "source": [
    "# ===== Block 3: One layer (10-way) classification =====\n",
    "\n",
    "class OneLayerMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)  # 784 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "model_1 = OneLayerMNIST().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)\n",
    "\n",
    "def eval_multiclass(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "test_loss, test_acc = eval_multiclass(model_1, test_loader)\n",
    "print(f\"Epoch 0 | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model_1.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_1(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_loss, test_acc = eval_multiclass(model_1, test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Export weights (float)\n",
    "W1 = model_1.fc.weight.detach().cpu().numpy()  # (10,784)\n",
    "b1 = model_1.fc.bias.detach().cpu().numpy()    # (10,)\n",
    "np.savez(\"mnist_lone_layer_weights.npz\", W=W1, b_bin=b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input value in hexa : 0.06 -> 7 -> 0x7\n",
      "\n",
      " expected output values (float -> int8 -> hexa) :\n",
      "0.177677 -> 2910 -> 0xb5e\n",
      "-0.706889 -> -11580 -> 0xd2c4\n",
      "0.347964 -> 5700 -> 0x1644\n",
      "0.260581 -> 4269 -> 0x10ad\n",
      "-0.200229 -> -3280 -> 0xf330\n",
      "0.312143 -> 5113 -> 0x13f9\n",
      "-0.017214 -> -282 -> 0xfee6\n",
      "-0.068261 -> -1118 -> 0xfba2\n",
      "0.183052 -> 2998 -> 0xbb6\n",
      "-0.215809 -> -3535 -> 0xf231\n",
      "\n",
      " results for saved input tensor([[-0.3827,  3.2244,  1.2534,  1.3987, -1.5689,  0.3251, -1.8647, -2.9003,\n",
      "          2.6056, -1.5241]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "-0.382704 -> -6269 -> 0xe783\n",
      "3.224383 -> 16382 -> 0x3ffe\n",
      "1.253367 -> 16382 -> 0x3ffe\n",
      "1.398744 -> 16382 -> 0x3ffe\n",
      "-1.568905 -> -16383 -> 0xc001\n",
      "0.325065 -> 5325 -> 0x14cd\n",
      "-1.864678 -> -16383 -> 0xc001\n",
      "-2.900279 -> -16383 -> 0xc001\n",
      "2.605643 -> 16382 -> 0x3ffe\n",
      "-1.524106 -> -16383 -> 0xc001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7628/2529267476.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_1.fc.bias = torch.nn.Parameter(torch.tensor(torch.zeros(10).to(device), dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "# SET THE BIAS TO 0 AND GET THE VALUE NEURON BY NEURON\n",
    "# BONUS : TEST THE OUTPUT VALUES FOR A FIXED INPUT\n",
    "\n",
    "test_input_value = (torch.ones((28*28))*0.0625).to(device)\n",
    "int8_convert_ratio = (2**(8-1))-1\n",
    "result_convert_ratio = (2**(15-1))-1\n",
    "print(f\"input value in hexa : {test_input_value[0]:.2f} -> {int(test_input_value[0]*int8_convert_ratio)} -> {hex(int(test_input_value[0]*int8_convert_ratio))}\")\n",
    "\n",
    "model_1.fc.bias = torch.nn.Parameter(torch.tensor(torch.zeros(10).to(device), dtype=torch.float32))\n",
    "out_test = model_1(test_input_value.view(1,1,28,28))\n",
    "# convert to integer between -128 and 127 for VHDL simulation where -128 is -1 and 127 is almost 1\n",
    "converted_out_test= out_test.clamp(-1, 0.9921875)  # clamp to avoid overflow\n",
    "out_test_int = (converted_out_test * result_convert_ratio).to(torch.int16)\n",
    "print(\"\\n expected output values (float -> int8 -> hexa) :\")\n",
    "for i in range(10) : \n",
    "    val_float = float(out_test[0][i])\n",
    "    val_int = int(out_test_int[0][i])\n",
    "    if out_test_int[0][i] < 0 :\n",
    "        print(f\"{val_float:2f} -> {val_int} -> {hex((2**16 + val_int))}\")\n",
    "    else :\n",
    "        print(f\"{val_float:2f} -> {val_int} -> {hex((val_int))}\")\n",
    "\n",
    "\n",
    "print(\"\\nresults for saved input\", model_1(saved_input_sample))\n",
    "for i in range(10) : \n",
    "    val_float = float(model_1(saved_input_sample)[0][i])\n",
    "    val_int = int((model_1(saved_input_sample)[0][i] * result_convert_ratio).clamp(-result_convert_ratio, result_convert_ratio-1))\n",
    "    if val_int < 0 :\n",
    "        print(f\"{val_float:2f} -> {val_int} -> {hex((2**16 + val_int))}\")\n",
    "    else :\n",
    "        print(f\"{val_float:2f} -> {val_int} -> {hex((val_int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf8799",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saved_input_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m     w_hex = w1\n\u001b[32m     14\u001b[39m saved_input_coe = [\u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x18\u001b[39m, \u001b[32m0x59\u001b[39m, \u001b[32m0x7F\u001b[39m, \u001b[32m0x09\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x02\u001b[39m, \u001b[32m0x51\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7C\u001b[39m, \u001b[32m0x08\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x03\u001b[39m, \u001b[32m0x50\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7B\u001b[39m, \u001b[32m0x26\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x2F\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x47\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x1E\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x79\u001b[39m, \u001b[32m0x0D\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x67\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x78\u001b[39m, \u001b[32m0x26\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x50\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x77\u001b[39m, \u001b[32m0x27\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x2B\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x4A\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x40\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x56\u001b[39m, \u001b[32m0x07\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x22\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x76\u001b[39m, \u001b[32m0x15\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x11\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x76\u001b[39m, \u001b[32m0x25\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x18\u001b[39m, \u001b[32m0x53\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x64\u001b[39m, \u001b[32m0x25\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x5F\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x76\u001b[39m, \u001b[32m0x19\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x04\u001b[39m, \u001b[32m0x54\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x77\u001b[39m, \u001b[32m0x25\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x23\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x52\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x04\u001b[39m, \u001b[32m0x5F\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x56\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x65\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x66\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x09\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x6C\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x09\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x79\u001b[39m, \u001b[32m0x44\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x09\u001b[39m, \u001b[32m0x66\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x36\u001b[39m, \u001b[32m0x01\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m saved_input_sample_flatten = \u001b[43msaved_input_sample\u001b[49m.view(-\u001b[32m1\u001b[39m).cpu().numpy().tolist()\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(int8_convert_ratio, result_convert_ratio)\n\u001b[32m     17\u001b[39m w_int = (model_1.fc.weight.detach().cpu()[neuron_idx]*int8_convert_ratio).to(torch.int8)\n",
      "\u001b[31mNameError\u001b[39m: name 'saved_input_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# SIMULATE THE VHDL NEURONS\n",
    "# BONUS 2 : TEST VHDL WEIGHTS CONVERSION \n",
    "\n",
    "from quantization_functions import int_to_vhdl_hex\n",
    "w0 = [0x00, 0xFD, 0x04, 0x00, 0x01, 0xFC, 0x02, 0xFF, 0xFD, 0x00, 0x02, 0x03, 0xFE, 0xFE, 0x00, 0x01, 0x01, 0x02, 0xFF, 0x02, 0x03, 0xFF, 0x03, 0x02, 0x04, 0xFE, 0xFE, 0xFF, 0x00, 0x02, 0xFE, 0x02, 0xFF, 0x02, 0x00, 0xFD, 0xFE, 0xFD, 0x02, 0xFD, 0x00, 0x00, 0xFE, 0x01, 0xFC, 0xFD, 0xFC, 0xFF, 0xFF, 0x00, 0x00, 0x02, 0x04, 0x01, 0xFF, 0xFC, 0x01, 0xFE, 0x02, 0x03, 0x01, 0x04, 0x04, 0x00, 0xFD, 0x03, 0xFE, 0xFE, 0x03, 0x02, 0x00, 0xFF, 0xFB, 0xFF, 0x03, 0x00, 0x00, 0xFF, 0xFF, 0x02, 0x00, 0x02, 0x00, 0x01, 0x01, 0x00, 0x02, 0x02, 0x04, 0x03, 0xFF, 0xFC, 0x00, 0xFF, 0xFE, 0xFE, 0xFF, 0x00, 0xFF, 0xFB, 0xFB, 0xFB, 0xFA, 0x00, 0x00, 0x01, 0xFF, 0x00, 0x01, 0xFC, 0x00, 0x00, 0xFE, 0xFD, 0x04, 0x03, 0x02, 0x03, 0xFE, 0xFE, 0x00, 0xFD, 0xFC, 0xFA, 0xFF, 0xFE, 0x01, 0x06, 0x00, 0xFF, 0x01, 0xFD, 0xFE, 0x01, 0xFD, 0xFC, 0xFD, 0xFF, 0xFE, 0xFE, 0x01, 0x00, 0xFF, 0xFD, 0xFF, 0xFE, 0xFD, 0xFA, 0xFD, 0xF8, 0xF9, 0xFB, 0x02, 0x02, 0x06, 0x0D, 0x0C, 0x0B, 0x01, 0x00, 0xFF, 0x02, 0xFF, 0xFA, 0xFF, 0xFC, 0xFF, 0xFF, 0x03, 0x01, 0x00, 0x00, 0xFF, 0x00, 0x00, 0xFE, 0xF9, 0xFF, 0xFD, 0xFF, 0x05, 0x09, 0x0C, 0x0C, 0x0B, 0x11, 0x10, 0x04, 0x04, 0x00, 0x00, 0xFF, 0xFC, 0xFE, 0x01, 0x00, 0xFD, 0x03, 0xFE, 0x00, 0x00, 0xFE, 0xF9, 0xFE, 0xF9, 0xFB, 0x00, 0xFF, 0x01, 0x03, 0x05, 0x08, 0x0C, 0x12, 0x0D, 0x09, 0x06, 0x04, 0x05, 0xFC, 0xFE, 0x00, 0x01, 0xFC, 0xFF, 0xFD, 0xFD, 0x01, 0xFE, 0x00, 0xFA, 0xFD, 0xFA, 0x00, 0xFE, 0x00, 0x00, 0x05, 0x0B, 0x0A, 0x09, 0x0E, 0x0F, 0x09, 0x0B, 0x05, 0x06, 0x00, 0xFC, 0xFE, 0x00, 0x00, 0xFE, 0x04, 0x00, 0xFD, 0xFC, 0x01, 0xFD, 0xFD, 0xFC, 0xFE, 0xFD, 0x00, 0x08, 0x0A, 0x09, 0x00, 0x07, 0x05, 0x0F, 0x10, 0x0D, 0x0B, 0x0C, 0x07, 0x02, 0xFD, 0x01, 0xFF, 0x00, 0xFD, 0xFF, 0xFD, 0xFD, 0xFB, 0xFB, 0x00, 0x01, 0x04, 0x04, 0x03, 0x02, 0x05, 0x01, 0xF7, 0xFB, 0x02, 0x07, 0x08, 0x10, 0x0E, 0x12, 0x09, 0x05, 0xFC, 0x04, 0xFF, 0x02, 0x00, 0x00, 0xFF, 0x00, 0xFD, 0xFD, 0x00, 0x04, 0x00, 0x05, 0x05, 0x07, 0x00, 0xF6, 0xEC, 0xE9, 0xF3, 0xFB, 0x0A, 0x08, 0x12, 0x18, 0x12, 0x07, 0xFC, 0x04, 0x00, 0xFC, 0x04, 0xFE, 0xFF, 0x01, 0xFE, 0x01, 0x05, 0x02, 0x05, 0x04, 0x07, 0xFE, 0xF2, 0xE5, 0xE6, 0xE3, 0xE6, 0xF9, 0x02, 0x0A, 0x0E, 0x16, 0x16, 0x0A, 0xFD, 0x04, 0x00, 0x02, 0x03, 0x03, 0xFC, 0x02, 0x00, 0x08, 0x05, 0x06, 0x09, 0x0A, 0x02, 0xF8, 0xEE, 0xE4, 0xDF, 0xDF, 0xE8, 0xF1, 0xFE, 0x07, 0x0C, 0x11, 0x12, 0x06, 0x00, 0xFF, 0x00, 0x02, 0xFF, 0x03, 0x02, 0xFC, 0x02, 0x07, 0x0A, 0x0A, 0x0D, 0x09, 0xFD, 0xEE, 0xE1, 0xE1, 0xD9, 0xD9, 0xE8, 0xF3, 0x01, 0x08, 0x11, 0x0E, 0x13, 0x0B, 0xFF, 0x04, 0x03, 0x00, 0x02, 0xFF, 0xFC, 0x00, 0x0A, 0x0B, 0x11, 0x0C, 0x14, 0x0D, 0xFB, 0xE7, 0xE1, 0xDB, 0xDB, 0xE1, 0xEF, 0xF7, 0x00, 0x07, 0x0B, 0x12, 0x0B, 0x06, 0x00, 0xFF, 0x03, 0x00, 0xFC, 0x00, 0xFD, 0x00, 0x09, 0x0B, 0x10, 0x12, 0x0F, 0x0B, 0xF6, 0xEB, 0xE0, 0xD8, 0xE0, 0xEB, 0xF1, 0xFC, 0x08, 0x0A, 0x06, 0x0D, 0x09, 0x00, 0x00, 0x01, 0xFF, 0x01, 0xFD, 0x03, 0x02, 0x02, 0x0A, 0x11, 0x0E, 0x11, 0x10, 0x04, 0xF4, 0xEE, 0xE6, 0xDD, 0xE8, 0xF3, 0x00, 0x05, 0x09, 0x03, 0x09, 0x0C, 0x01, 0xFF, 0x00, 0x02, 0xFE, 0x00, 0x00, 0x00, 0xFC, 0x02, 0x0B, 0x0A, 0x10, 0x12, 0x12, 0x0D, 0xFD, 0xF0, 0xEF, 0xEA, 0xF1, 0xFB, 0xFE, 0x04, 0x00, 0x03, 0x04, 0x05, 0x00, 0xFD, 0x01, 0x03, 0xFC, 0x01, 0xFD, 0x00, 0x02, 0x02, 0x02, 0x0D, 0x10, 0x10, 0x15, 0x0A, 0x01, 0xF9, 0xF7, 0xF9, 0xFD, 0xFF, 0x01, 0x04, 0xFE, 0x00, 0x02, 0x00, 0x01, 0x00, 0x02, 0x00, 0xFE, 0x03, 0x00, 0xFF, 0xFC, 0x00, 0x00, 0x08, 0x0B, 0x09, 0x0E, 0x0C, 0x0D, 0x06, 0x00, 0x03, 0x01, 0x00, 0x04, 0x01, 0x03, 0x00, 0x04, 0x02, 0x01, 0xFB, 0x03, 0xFD, 0x04, 0x00, 0xFC, 0x01, 0xFE, 0xFC, 0x00, 0x05, 0x0A, 0x08, 0x11, 0x11, 0x0F, 0x0A, 0x03, 0x02, 0x04, 0x00, 0xFF, 0xFF, 0xFB, 0xFD, 0xFE, 0xFE, 0xFD, 0x00, 0x00, 0x01, 0x00, 0xFE, 0xFF, 0x00, 0xFD, 0xFF, 0x00, 0x05, 0x06, 0x0C, 0x0C, 0x0D, 0x14, 0x0E, 0x0F, 0x0A, 0x06, 0x01, 0x03, 0xFF, 0xFE, 0x00, 0xF9, 0xFF, 0x02, 0x02, 0xFD, 0x01, 0xFE, 0x00, 0x00, 0xFD, 0xFC, 0xFD, 0x01, 0x02, 0xFF, 0x02, 0x07, 0x07, 0x10, 0x12, 0x0D, 0x04, 0x04, 0x00, 0x00, 0xF9, 0xF7, 0xFE, 0xFE, 0xFE, 0xFE, 0xFE, 0x03, 0x00, 0x03, 0x04, 0x00, 0x00, 0x00, 0x02, 0xFC, 0x00, 0xFE, 0x00, 0xFE, 0xFC, 0xFD, 0x00, 0xFC, 0xFB, 0xFB, 0xF9, 0xFD, 0xFF, 0xFC, 0x01, 0xFD, 0xFD, 0xFE, 0xFD, 0xFD, 0xFC, 0xFC, 0x01, 0x02, 0x04, 0x03, 0x00, 0x00, 0xFD, 0x00, 0xFA, 0xFF, 0xFC, 0xFD, 0xFD, 0xF9, 0xFF, 0xFE, 0xFF, 0xFD, 0x00, 0x01, 0x00, 0x02, 0xFF, 0x02, 0x01, 0x02, 0x04, 0x01, 0xFF, 0x00, 0x01, 0x00, 0xFF, 0x03, 0x02, 0xFD, 0xFF, 0xFF, 0xFC, 0xFF, 0xFD, 0xFC, 0xFD, 0xFD, 0x02, 0x00, 0x03, 0x00, 0x00, 0x01, 0x00, 0xFD, 0xFE, 0xFF, 0x00, 0x01, 0x01, 0x03, 0x00, 0xFF, 0xFD, 0xFD, 0xFE, 0x01, 0x02, 0x03, 0xFF, 0x00, 0xFF, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0xFF, 0xFE, 0x01, 0x00, 0xFE, 0x01, 0x00, 0x00, 0xFE]\n",
    "w1 = [0x00, 0xFD, 0x01, 0x00, 0x02, 0x00, 0x00, 0x01, 0x03, 0xFE, 0x02, 0x01, 0xFD, 0x01, 0xFE, 0xFF, 0xFE, 0xFD, 0x03, 0xFC, 0x02, 0xFC, 0x00, 0x00, 0x03, 0xFF, 0x01, 0x00, 0x02, 0x00, 0x01, 0xFE, 0xFD, 0x04, 0xFD, 0x00, 0x00, 0x04, 0xFF, 0xFD, 0xFF, 0x02, 0x01, 0x03, 0x00, 0x02, 0xFC, 0x01, 0x02, 0x02, 0xFE, 0xFF, 0x01, 0xFD, 0xFD, 0xFC, 0x00, 0x02, 0x01, 0x00, 0x01, 0xFE, 0x00, 0xFF, 0xFE, 0x03, 0xFD, 0x00, 0xFC, 0x00, 0xFE, 0x01, 0xFE, 0x02, 0xFC, 0x02, 0xFD, 0x02, 0xFE, 0x00, 0x00, 0x02, 0x01, 0xFD, 0x00, 0xFD, 0x03, 0x00, 0xFF, 0xFE, 0x01, 0xFE, 0x01, 0x01, 0x00, 0xFF, 0xFA, 0x01, 0xFD, 0xFE, 0xFE, 0x00, 0x00, 0xFF, 0x00, 0xFB, 0x00, 0xFC, 0x01, 0xFD, 0xFF, 0x03, 0x04, 0xFE, 0xFE, 0x00, 0x01, 0x00, 0xFF, 0xFC, 0x00, 0x01, 0xFB, 0x00, 0x03, 0x08, 0x04, 0x03, 0x04, 0x02, 0x05, 0x03, 0x08, 0x08, 0x00, 0x01, 0xFF, 0x02, 0xFE, 0x00, 0x02, 0xFC, 0xFD, 0xFE, 0x02, 0x02, 0xFC, 0xFF, 0xF9, 0xF8, 0xFE, 0xFF, 0x00, 0x07, 0x07, 0x04, 0xFE, 0xFC, 0x04, 0x07, 0x0C, 0x07, 0x07, 0x04, 0x02, 0x00, 0x04, 0x00, 0x00, 0x04, 0x04, 0xFF, 0x03, 0x00, 0xFE, 0x00, 0xF6, 0xF8, 0xF9, 0xF4, 0xFA, 0xFD, 0x00, 0xF7, 0xF5, 0xF9, 0x01, 0x07, 0x0A, 0x06, 0xFF, 0xFC, 0xFF, 0x03, 0x00, 0x00, 0xFD, 0xFE, 0xFD, 0x03, 0xFF, 0xFB, 0xFD, 0xFA, 0xF5, 0xF3, 0xF2, 0xED, 0xF3, 0xFC, 0xF9, 0xF5, 0xF9, 0xF7, 0xFB, 0x00, 0xFE, 0x01, 0xFB, 0xF9, 0xF9, 0x00, 0x00, 0x00, 0xFF, 0xFE, 0x00, 0x03, 0xFD, 0xFF, 0xF8, 0xF6, 0xF8, 0xF0, 0xF2, 0xED, 0xF2, 0xFB, 0x06, 0x00, 0x00, 0xFF, 0x00, 0xFF, 0xF8, 0xFA, 0xF4, 0xF7, 0xFE, 0xFF, 0x00, 0xFF, 0xFE, 0x02, 0x00, 0x00, 0x00, 0x01, 0xFA, 0xF8, 0xF8, 0xF4, 0xEC, 0xEA, 0xF6, 0x03, 0x0F, 0x15, 0x05, 0x02, 0xF9, 0xFA, 0xF7, 0xF5, 0xFA, 0xF6, 0xF8, 0xFA, 0x02, 0x01, 0xFD, 0x02, 0x04, 0xFC, 0x02, 0xFF, 0xFD, 0xFE, 0xFB, 0xF4, 0xF3, 0xF0, 0xF5, 0x02, 0x1D, 0x1D, 0x10, 0x00, 0xF8, 0xF1, 0xF0, 0xF9, 0xFB, 0xFE, 0xFC, 0xFF, 0xFF, 0x03, 0xFF, 0x02, 0x03, 0xFC, 0x00, 0xFE, 0xFB, 0xFD, 0xF7, 0xF6, 0xEB, 0xED, 0xEE, 0x08, 0x29, 0x2D, 0x10, 0xF8, 0xF6, 0xF0, 0xF6, 0xF5, 0xFB, 0x00, 0xFB, 0xFC, 0x00, 0x00, 0x00, 0x03, 0xFE, 0xFE, 0xFD, 0xFE, 0xFC, 0xFE, 0xF7, 0xF3, 0xE9, 0xEB, 0xF2, 0x0B, 0x32, 0x26, 0x09, 0xF2, 0xF0, 0xF8, 0xF4, 0xFF, 0xFE, 0x01, 0xFD, 0x02, 0x00, 0x00, 0x03, 0xFC, 0xFD, 0x04, 0xFE, 0xFC, 0xFD, 0xF9, 0xF7, 0xEE, 0xEA, 0xE0, 0xEE, 0x1A, 0x36, 0x24, 0xFF, 0xED, 0xEE, 0xF2, 0xF6, 0xF9, 0x01, 0xFE, 0x00, 0x02, 0x00, 0xFE, 0xFD, 0xFF, 0x01, 0xFD, 0x00, 0x00, 0x00, 0xFC, 0xF7, 0xF3, 0xEA, 0xE5, 0xF2, 0x1C, 0x30, 0x1A, 0xF4, 0xE8, 0xEF, 0xF7, 0xF6, 0xF7, 0xFB, 0xFC, 0x01, 0xFD, 0x00, 0x00, 0xFE, 0x02, 0xFF, 0x00, 0xFD, 0xFE, 0xFD, 0xFA, 0xF4, 0xF1, 0xE8, 0xEA, 0xFD, 0x23, 0x29, 0x13, 0xEA, 0xE4, 0xED, 0xF4, 0xF7, 0xFD, 0xFA, 0xFF, 0xFF, 0xFC, 0xFF, 0x01, 0x00, 0x03, 0x03, 0xFE, 0xFE, 0xFD, 0xF8, 0xF7, 0xF1, 0xF3, 0xF3, 0xF8, 0x05, 0x21, 0x25, 0x05, 0xEA, 0xE7, 0xF1, 0xF4, 0xF8, 0xF9, 0xFE, 0xFF, 0xFB, 0xFD, 0x00, 0x01, 0x02, 0xFE, 0xFC, 0x00, 0xFF, 0xFE, 0xF8, 0xFA, 0xF5, 0xF7, 0xF6, 0x02, 0x10, 0x20, 0x1E, 0xFB, 0xF0, 0xE9, 0xEE, 0xF5, 0xF7, 0xFC, 0xFF, 0xFA, 0x00, 0xFF, 0xFF, 0x00, 0x03, 0xFE, 0xFD, 0x02, 0xFD, 0xFD, 0xF6, 0xF6, 0xF9, 0xF7, 0xFD, 0x09, 0x11, 0x18, 0x0D, 0x01, 0xF0, 0xEC, 0xEE, 0xF3, 0xFA, 0xFA, 0xFF, 0xFD, 0xFE, 0x01, 0x00, 0x00, 0x01, 0x03, 0x01, 0x02, 0x01, 0xFC, 0xF8, 0xF7, 0xF8, 0x00, 0x06, 0x09, 0x0A, 0x11, 0x05, 0x01, 0xF7, 0xF4, 0xF3, 0xF3, 0xFC, 0xF9, 0xFD, 0x00, 0xFB, 0x00, 0xFF, 0x03, 0xFE, 0x02, 0x02, 0x01, 0x02, 0xFD, 0xF6, 0xFD, 0xFD, 0x01, 0x00, 0x02, 0x00, 0x06, 0x05, 0x01, 0xFD, 0xF4, 0xF5, 0xF6, 0xF8, 0xFF, 0xFE, 0x01, 0xFF, 0xFC, 0xFF, 0x03, 0x02, 0x01, 0xFF, 0xFC, 0xFD, 0x01, 0xFE, 0x02, 0x03, 0x01, 0x04, 0xFB, 0xF8, 0xF8, 0xFB, 0x01, 0x00, 0xFA, 0xFD, 0xF6, 0xF7, 0xFA, 0xFC, 0xFE, 0xFF, 0x01, 0xFC, 0x00, 0x00, 0x01, 0x04, 0x02, 0xFE, 0xFF, 0x03, 0x03, 0x08, 0x04, 0x02, 0xF8, 0xF1, 0xF7, 0x00, 0x04, 0x08, 0xFE, 0x00, 0xFA, 0xF9, 0xFD, 0x00, 0x02, 0x00, 0x00, 0x00, 0xFF, 0x00, 0x00, 0x03, 0x02, 0xFD, 0x02, 0x02, 0x06, 0x03, 0x05, 0xFA, 0xFB, 0xF9, 0xF6, 0xFD, 0x06, 0x05, 0x04, 0xFC, 0x00, 0xFB, 0xFD, 0xFD, 0x01, 0x00, 0xFC, 0xFE, 0xFE, 0xFC, 0x00, 0x01, 0x00, 0xFF, 0x00, 0xFE, 0xFE, 0xFE, 0x00, 0xF6, 0xF7, 0xF3, 0xF5, 0xF5, 0x00, 0x01, 0xFF, 0x01, 0x02, 0x01, 0x01, 0xFC, 0xFC, 0xFD, 0x01, 0xFD, 0xFD, 0xFE, 0xFC, 0x00, 0x00, 0xFE, 0x02, 0xFE, 0xFD, 0x00, 0x00, 0xFA, 0xF7, 0xF8, 0xFB, 0xFE, 0xFE, 0xFC, 0xFB, 0xFC, 0xFF, 0x00, 0xFE, 0xFD, 0x00, 0xFD, 0xFD, 0xFD, 0xFF, 0x04, 0x01, 0x00, 0x00, 0x01, 0x00, 0xFD, 0xFD, 0xFD, 0x00, 0xFB, 0xFE, 0xFD, 0x02, 0x02, 0x01, 0x00, 0xFF, 0x00, 0xFD, 0xFE, 0x00, 0xFF, 0xFC, 0x01, 0x03, 0xFE, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 0xFE, 0xFF, 0x02, 0x01, 0x00, 0x00, 0xFF, 0xFF, 0xFE, 0x00, 0x01, 0x03, 0xFF, 0x00, 0x02, 0x01, 0x00, 0x01, 0x00, 0xFD, 0x00, 0x00, 0x04]\n",
    "\n",
    "neuron_idx = 1\n",
    "if neuron_idx == 0 : \n",
    "    w_hex = w0\n",
    "else :\n",
    "    w_hex = w1\n",
    "\n",
    "\n",
    "saved_input_coe = [0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x18, 0x59, 0x7F, 0x09, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x51, 0x7E, 0x7C, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x50, 0x7E, 0x7B, 0x26, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x2F, 0x7E, 0x7E, 0x47, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x1E, 0x75, 0x7E, 0x79, 0x0D, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x67, 0x7E, 0x78, 0x26, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x50, 0x7E, 0x77, 0x27, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x2B, 0x7E, 0x7E, 0x4A, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x40, 0x75, 0x7E, 0x56, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x22, 0x75, 0x7E, 0x76, 0x15, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x11, 0x75, 0x7E, 0x76, 0x25, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x18, 0x53, 0x7E, 0x64, 0x25, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x5F, 0x7E, 0x76, 0x19, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x54, 0x7E, 0x77, 0x25, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x23, 0x7E, 0x7E, 0x52, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x5F, 0x7E, 0x56, 0x05, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x65, 0x7E, 0x66, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x7E, 0x7E, 0x6C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x7E, 0x7E, 0x7E, 0x79, 0x44, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x66, 0x75, 0x36, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00]\n",
    "saved_input_sample_flatten = saved_input_sample.view(-1).cpu().numpy().tolist()\n",
    "print(int8_convert_ratio, result_convert_ratio)\n",
    "w_int = (model_1.fc.weight.detach().cpu()[neuron_idx]*int8_convert_ratio).to(torch.int8)\n",
    "w_float = model_1.fc.weight.detach().cpu()[neuron_idx]\n",
    "res = 0\n",
    "res_int = 0\n",
    "res_float = 0.0\n",
    "for i, w in enumerate(w_hex) : \n",
    "    if w > 127 :\n",
    "        w = w - 256\n",
    "    res = res + w*saved_input_coe[i]\n",
    "    res_int = res_int + int(w_int[i])*saved_input_coe[i]\n",
    "    res_float = res_float + float(w_float[i])*saved_input_sample_flatten[i]\n",
    "\n",
    "hex_res = int_to_vhdl_hex(res,16)\n",
    "hex_res_int = int_to_vhdl_hex(res_int,16)\n",
    "hex_res_float = int_to_vhdl_hex(int(res_float*int8_convert_ratio),16)\n",
    "print(f\"hexa : {res}, {hex_res}, {res/result_convert_ratio:.4f}\")\n",
    "print(f\"int  : {res_int}, {hex_res_int}, {res_int/result_convert_ratio:.4f}\")\n",
    "print(f\"float: {res_float}, {(int(res_float*result_convert_ratio))}, {hex_res_float}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b007e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | test_loss=2.2990 | test_acc=12.96%\n",
      "Epoch 01 | test_loss=0.2253 | test_acc=93.49%\n",
      "Two-layer shapes: \n",
      "  W2_1 (128, 784) b2_1 (128,) \n",
      "  W2_2 (10, 128) b2_2 (10,)\n"
     ]
    }
   ],
   "source": [
    "# ===== Block 4: Two-layer MLP (128 -> 10) =====\n",
    "\n",
    "class TwoLayerMNIST(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden)  # 784 -> 128\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, 10)     # 128 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "model_2 = TwoLayerMNIST(hidden=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 1\n",
    "test_loss, test_acc = eval_multiclass(model_2, test_loader)\n",
    "print(f\"Epoch 0 | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model_2.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_2(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_loss, test_acc = eval_multiclass(model_2, test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Export weights (float)\n",
    "W2_1 = model_2.fc1.weight.detach().cpu().numpy()  # (128,784)\n",
    "b2_1 = model_2.fc1.bias.detach().cpu().numpy()    # (128,)\n",
    "W2_2 = model_2.fc2.weight.detach().cpu().numpy()  # (10,128)\n",
    "b2_2 = model_2.fc2.bias.detach().cpu().numpy()    # (10,)\n",
    "\n",
    "print(\"Two-layer shapes:\",\n",
    "      \"\\n  W2_1\", W2_1.shape, \"b2_1\", b2_1.shape,\n",
    "      \"\\n  W2_2\", W2_2.shape, \"b2_2\", b2_2.shape)\n",
    "\n",
    "np.savez(\"mnist_l1_weights.npz\", W=W2_1, b_bin=b2_1)\n",
    "np.savez(\"mnist_l2_weights.npz\", W=W2_2, b_bin=b2_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
