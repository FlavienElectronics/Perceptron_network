{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d283db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "c:\\ProgramData\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch x: torch.Size([128, 1, 28, 28]) Batch y: torch.Size([128]) Labels sample: [1, 2, 8, 5, 2, 6, 9, 9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- Block 1: Imports + Dataset -----\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get always the same results\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# MNIST: pixels between 0 and 1\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Quick sanity check: visualize a few samples\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch x:\", x.shape, \"Batch y:\", y.shape, \"Labels sample:\", y[:8].tolist())\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(10, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(x[i, 0].numpy(), cmap=\"gray\")\n",
    "    ax.set_title(str(y[i].item()))\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3885da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    }
   ],
   "source": [
    "saved_input_idx = 1\n",
    "saved_input_sample = x[saved_input_idx:saved_input_idx+1].to(device)\n",
    "saved_input_label  = y[saved_input_idx:saved_input_idx+1].to(device)\n",
    "plt.imshow(saved_input_sample[0, 0].cpu().numpy(), cmap=\"gray\")\n",
    "plt.title(f\"Label: {saved_input_label.item()}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Export input in a vector between 0 and 1 in a npz\n",
    "print(saved_input_sample.cpu().view(saved_input_sample.size(0), -1).numpy().shape)\n",
    "np.savez_compressed(\"mnist_sample_input.npz\", input=saved_input_sample.cpu().view(saved_input_sample.size(0), -1).numpy(), label=saved_input_label.cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baabf448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7628/3182847473.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | test_loss=0.6788 | test_acc=62.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7628/3182847473.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | test_loss=0.0415 | test_acc=98.97%\n"
     ]
    }
   ],
   "source": [
    "# ----- Block 2: One neuron Is it a 0 ?  -----\n",
    "\n",
    "# Create a new loader that map label (1 if 0 else 0)\n",
    "def make_binary_loaders(batch_size=128):\n",
    "    binary_target = lambda t: 1 if int(t) == 0 else 0\n",
    "\n",
    "    train_bin = datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform, target_transform=binary_target\n",
    "    )\n",
    "    test_bin = datasets.MNIST(\n",
    "        root=\"./data\", train=False, download=True, transform=transform, target_transform=binary_target\n",
    "    )\n",
    "\n",
    "    train_bin_loader = DataLoader(train_bin, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_bin_loader  = DataLoader(test_bin,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_bin_loader, test_bin_loader\n",
    "\n",
    "train_bin_loader, test_bin_loader = make_binary_loaders(BATCH_SIZE)\n",
    "\n",
    "class SingleNeuronBinary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 1)  # 784 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)      # flatten\n",
    "        out = self.fc(x)            \n",
    "        return out\n",
    "\n",
    "model_bin = SingleNeuronBinary().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()     # sigmoid included internally\n",
    "optimizer = torch.optim.SGD(model_bin.parameters(), lr=0.1)\n",
    "\n",
    "def eval_binary(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = torch.tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "            preds = (out >= 0).float()  # threshold at 0 logit\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "# Training loop\n",
    "test_loss, test_acc = eval_binary(model_bin, test_bin_loader)\n",
    "print(f\"Epoch 0 | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model_bin.train()\n",
    "    for x, y in train_bin_loader:\n",
    "        x = x.to(device)\n",
    "        y = torch.tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_bin(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_loss, test_acc = eval_binary(model_bin, test_bin_loader)\n",
    "    print(f\"Epoch {epoch:02d} | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Export weights for later VHDL conversion (float)\n",
    "W_bin = model_bin.fc.weight.detach().cpu().numpy()  # shape (1,784)\n",
    "b_bin = model_bin.fc.bias.detach().cpu().numpy()    # shape (1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986eb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | test_loss=2.3419 | test_acc=9.04%\n",
      "Epoch 01 | test_loss=0.3812 | test_acc=90.03%\n"
     ]
    }
   ],
   "source": [
    "# ===== Block 3: One layer (10-way) classification =====\n",
    "\n",
    "class OneLayerMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)  # 784 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "model_1 = OneLayerMNIST().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)\n",
    "\n",
    "def eval_multiclass(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "test_loss, test_acc = eval_multiclass(model_1, test_loader)\n",
    "print(f\"Epoch 0 | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model_1.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_1(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_loss, test_acc = eval_multiclass(model_1, test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Export weights (float)\n",
    "W1 = model_1.fc.weight.detach().cpu().numpy()  # (10,784)\n",
    "b1 = model_1.fc.bias.detach().cpu().numpy()    # (10,)\n",
    "np.savez(\"mnist_lone_layer_weights.npz\", W=W1, b_bin=b1)\n",
    "\n",
    "\n",
    "# FIN =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input value in hexa : 0.06 -> 7 -> 0x7\n",
      "\n",
      " expected output values (float -> int8 -> hexa) :\n",
      "0.177677 -> 2910 -> 0xb5e\n",
      "-0.706889 -> -11580 -> 0xd2c4\n",
      "0.347964 -> 5700 -> 0x1644\n",
      "0.260581 -> 4269 -> 0x10ad\n",
      "-0.200229 -> -3280 -> 0xf330\n",
      "0.312143 -> 5113 -> 0x13f9\n",
      "-0.017214 -> -282 -> 0xfee6\n",
      "-0.068261 -> -1118 -> 0xfba2\n",
      "0.183052 -> 2998 -> 0xbb6\n",
      "-0.215809 -> -3535 -> 0xf231\n",
      "\n",
      " results for saved input tensor([[-0.3827,  3.2244,  1.2534,  1.3987, -1.5689,  0.3251, -1.8647, -2.9003,\n",
      "          2.6056, -1.5241]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "-0.382704 -> -6269 -> 0xe783\n",
      "3.224383 -> 16382 -> 0x3ffe\n",
      "1.253367 -> 16382 -> 0x3ffe\n",
      "1.398744 -> 16382 -> 0x3ffe\n",
      "-1.568905 -> -16383 -> 0xc001\n",
      "0.325065 -> 5325 -> 0x14cd\n",
      "-1.864678 -> -16383 -> 0xc001\n",
      "-2.900279 -> -16383 -> 0xc001\n",
      "2.605643 -> 16382 -> 0x3ffe\n",
      "-1.524106 -> -16383 -> 0xc001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7628/2529267476.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_1.fc.bias = torch.nn.Parameter(torch.tensor(torch.zeros(10).to(device), dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "# SET THE BIAS TO 0 AND GET THE VALUE NEURON BY NEURON\n",
    "# BONUS : TEST THE OUTPUT VALUES FOR A FIXED INPUT\n",
    "\n",
    "test_input_value = (torch.ones((28*28))*0.0625).to(device)\n",
    "int8_convert_ratio = (2**(8-1))-1\n",
    "result_convert_ratio = (2**(15-1))-1\n",
    "print(f\"input value in hexa : {test_input_value[0]:.2f} -> {int(test_input_value[0]*int8_convert_ratio)} -> {hex(int(test_input_value[0]*int8_convert_ratio))}\")\n",
    "\n",
    "model_1.fc.bias = torch.nn.Parameter(torch.tensor(torch.zeros(10).to(device), dtype=torch.float32))\n",
    "out_test = model_1(test_input_value.view(1,1,28,28))\n",
    "# convert to integer between -128 and 127 for VHDL simulation where -128 is -1 and 127 is almost 1\n",
    "converted_out_test= out_test.clamp(-1, 0.9921875)  # clamp to avoid overflow\n",
    "out_test_int = (converted_out_test * result_convert_ratio).to(torch.int16)\n",
    "print(\"\\n expected output values (float -> int8 -> hexa) :\")\n",
    "for i in range(10) : \n",
    "    val_float = float(out_test[0][i])\n",
    "    val_int = int(out_test_int[0][i])\n",
    "    if out_test_int[0][i] < 0 :\n",
    "        print(f\"{val_float:2f} -> {val_int} -> {hex((2**16 + val_int))}\")\n",
    "    else :\n",
    "        print(f\"{val_float:2f} -> {val_int} -> {hex((val_int))}\")\n",
    "\n",
    "\n",
    "print(\"\\nresults for saved input\", model_1(saved_input_sample))\n",
    "for i in range(10) : \n",
    "    val_float = float(model_1(saved_input_sample)[0][i])\n",
    "    val_int = int((model_1(saved_input_sample)[0][i] * result_convert_ratio).clamp(-result_convert_ratio, result_convert_ratio-1))\n",
    "    if val_int < 0 :\n",
    "        print(f\"{val_float:2f} -> {val_int} -> {hex((2**16 + val_int))}\")\n",
    "    else :\n",
    "        print(f\"{val_float:2f} -> {val_int} -> {hex((val_int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf8799",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saved_input_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m     w_hex = w1\n\u001b[32m     14\u001b[39m saved_input_coe = [\u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x18\u001b[39m, \u001b[32m0x59\u001b[39m, \u001b[32m0x7F\u001b[39m, \u001b[32m0x09\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x02\u001b[39m, \u001b[32m0x51\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7C\u001b[39m, \u001b[32m0x08\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x03\u001b[39m, \u001b[32m0x50\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7B\u001b[39m, \u001b[32m0x26\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x2F\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x47\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x1E\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x79\u001b[39m, \u001b[32m0x0D\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x67\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x78\u001b[39m, \u001b[32m0x26\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x50\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x77\u001b[39m, \u001b[32m0x27\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x2B\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x4A\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x40\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x56\u001b[39m, \u001b[32m0x07\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x22\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x76\u001b[39m, \u001b[32m0x15\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x11\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x76\u001b[39m, \u001b[32m0x25\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x18\u001b[39m, \u001b[32m0x53\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x64\u001b[39m, \u001b[32m0x25\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x5F\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x76\u001b[39m, \u001b[32m0x19\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x04\u001b[39m, \u001b[32m0x54\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x77\u001b[39m, \u001b[32m0x25\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x23\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x52\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x04\u001b[39m, \u001b[32m0x5F\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x56\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x05\u001b[39m, \u001b[32m0x65\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x66\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x09\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x6C\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x09\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x7E\u001b[39m, \u001b[32m0x79\u001b[39m, \u001b[32m0x44\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x09\u001b[39m, \u001b[32m0x66\u001b[39m, \u001b[32m0x75\u001b[39m, \u001b[32m0x36\u001b[39m, \u001b[32m0x01\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m, \u001b[32m0x00\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m saved_input_sample_flatten = \u001b[43msaved_input_sample\u001b[49m.view(-\u001b[32m1\u001b[39m).cpu().numpy().tolist()\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(int8_convert_ratio, result_convert_ratio)\n\u001b[32m     17\u001b[39m w_int = (model_1.fc.weight.detach().cpu()[neuron_idx]*int8_convert_ratio).to(torch.int8)\n",
      "\u001b[31mNameError\u001b[39m: name 'saved_input_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# SIMULATE THE VHDL NEURONS\n",
    "# BONUS 2 : TEST VHDL WEIGHTS CONVERSION \n",
    "\n",
    "from quantization_functions import int_to_vhdl_hex\n",
    "w0 = [0x00, 0xFD, 0x04, 0x00, 0x01, 0xFC, 0x02, 0xFF, 0xFD, 0x00, 0x02, 0x03, 0xFE, 0xFE, 0x00, 0x01, 0x01, 0x02, 0xFF, 0x02, 0x03, 0xFF, 0x03, 0x02, 0x04, 0xFE, 0xFE, 0xFF, 0x00, 0x02, 0xFE, 0x02, 0xFF, 0x02, 0x00, 0xFD, 0xFE, 0xFD, 0x02, 0xFD, 0x00, 0x00, 0xFE, 0x01, 0xFC, 0xFD, 0xFC, 0xFF, 0xFF, 0x00, 0x00, 0x02, 0x04, 0x01, 0xFF, 0xFC, 0x01, 0xFE, 0x02, 0x03, 0x01, 0x04, 0x04, 0x00, 0xFD, 0x03, 0xFE, 0xFE, 0x03, 0x02, 0x00, 0xFF, 0xFB, 0xFF, 0x03, 0x00, 0x00, 0xFF, 0xFF, 0x02, 0x00, 0x02, 0x00, 0x01, 0x01, 0x00, 0x02, 0x02, 0x04, 0x03, 0xFF, 0xFC, 0x00, 0xFF, 0xFE, 0xFE, 0xFF, 0x00, 0xFF, 0xFB, 0xFB, 0xFB, 0xFA, 0x00, 0x00, 0x01, 0xFF, 0x00, 0x01, 0xFC, 0x00, 0x00, 0xFE, 0xFD, 0x04, 0x03, 0x02, 0x03, 0xFE, 0xFE, 0x00, 0xFD, 0xFC, 0xFA, 0xFF, 0xFE, 0x01, 0x06, 0x00, 0xFF, 0x01, 0xFD, 0xFE, 0x01, 0xFD, 0xFC, 0xFD, 0xFF, 0xFE, 0xFE, 0x01, 0x00, 0xFF, 0xFD, 0xFF, 0xFE, 0xFD, 0xFA, 0xFD, 0xF8, 0xF9, 0xFB, 0x02, 0x02, 0x06, 0x0D, 0x0C, 0x0B, 0x01, 0x00, 0xFF, 0x02, 0xFF, 0xFA, 0xFF, 0xFC, 0xFF, 0xFF, 0x03, 0x01, 0x00, 0x00, 0xFF, 0x00, 0x00, 0xFE, 0xF9, 0xFF, 0xFD, 0xFF, 0x05, 0x09, 0x0C, 0x0C, 0x0B, 0x11, 0x10, 0x04, 0x04, 0x00, 0x00, 0xFF, 0xFC, 0xFE, 0x01, 0x00, 0xFD, 0x03, 0xFE, 0x00, 0x00, 0xFE, 0xF9, 0xFE, 0xF9, 0xFB, 0x00, 0xFF, 0x01, 0x03, 0x05, 0x08, 0x0C, 0x12, 0x0D, 0x09, 0x06, 0x04, 0x05, 0xFC, 0xFE, 0x00, 0x01, 0xFC, 0xFF, 0xFD, 0xFD, 0x01, 0xFE, 0x00, 0xFA, 0xFD, 0xFA, 0x00, 0xFE, 0x00, 0x00, 0x05, 0x0B, 0x0A, 0x09, 0x0E, 0x0F, 0x09, 0x0B, 0x05, 0x06, 0x00, 0xFC, 0xFE, 0x00, 0x00, 0xFE, 0x04, 0x00, 0xFD, 0xFC, 0x01, 0xFD, 0xFD, 0xFC, 0xFE, 0xFD, 0x00, 0x08, 0x0A, 0x09, 0x00, 0x07, 0x05, 0x0F, 0x10, 0x0D, 0x0B, 0x0C, 0x07, 0x02, 0xFD, 0x01, 0xFF, 0x00, 0xFD, 0xFF, 0xFD, 0xFD, 0xFB, 0xFB, 0x00, 0x01, 0x04, 0x04, 0x03, 0x02, 0x05, 0x01, 0xF7, 0xFB, 0x02, 0x07, 0x08, 0x10, 0x0E, 0x12, 0x09, 0x05, 0xFC, 0x04, 0xFF, 0x02, 0x00, 0x00, 0xFF, 0x00, 0xFD, 0xFD, 0x00, 0x04, 0x00, 0x05, 0x05, 0x07, 0x00, 0xF6, 0xEC, 0xE9, 0xF3, 0xFB, 0x0A, 0x08, 0x12, 0x18, 0x12, 0x07, 0xFC, 0x04, 0x00, 0xFC, 0x04, 0xFE, 0xFF, 0x01, 0xFE, 0x01, 0x05, 0x02, 0x05, 0x04, 0x07, 0xFE, 0xF2, 0xE5, 0xE6, 0xE3, 0xE6, 0xF9, 0x02, 0x0A, 0x0E, 0x16, 0x16, 0x0A, 0xFD, 0x04, 0x00, 0x02, 0x03, 0x03, 0xFC, 0x02, 0x00, 0x08, 0x05, 0x06, 0x09, 0x0A, 0x02, 0xF8, 0xEE, 0xE4, 0xDF, 0xDF, 0xE8, 0xF1, 0xFE, 0x07, 0x0C, 0x11, 0x12, 0x06, 0x00, 0xFF, 0x00, 0x02, 0xFF, 0x03, 0x02, 0xFC, 0x02, 0x07, 0x0A, 0x0A, 0x0D, 0x09, 0xFD, 0xEE, 0xE1, 0xE1, 0xD9, 0xD9, 0xE8, 0xF3, 0x01, 0x08, 0x11, 0x0E, 0x13, 0x0B, 0xFF, 0x04, 0x03, 0x00, 0x02, 0xFF, 0xFC, 0x00, 0x0A, 0x0B, 0x11, 0x0C, 0x14, 0x0D, 0xFB, 0xE7, 0xE1, 0xDB, 0xDB, 0xE1, 0xEF, 0xF7, 0x00, 0x07, 0x0B, 0x12, 0x0B, 0x06, 0x00, 0xFF, 0x03, 0x00, 0xFC, 0x00, 0xFD, 0x00, 0x09, 0x0B, 0x10, 0x12, 0x0F, 0x0B, 0xF6, 0xEB, 0xE0, 0xD8, 0xE0, 0xEB, 0xF1, 0xFC, 0x08, 0x0A, 0x06, 0x0D, 0x09, 0x00, 0x00, 0x01, 0xFF, 0x01, 0xFD, 0x03, 0x02, 0x02, 0x0A, 0x11, 0x0E, 0x11, 0x10, 0x04, 0xF4, 0xEE, 0xE6, 0xDD, 0xE8, 0xF3, 0x00, 0x05, 0x09, 0x03, 0x09, 0x0C, 0x01, 0xFF, 0x00, 0x02, 0xFE, 0x00, 0x00, 0x00, 0xFC, 0x02, 0x0B, 0x0A, 0x10, 0x12, 0x12, 0x0D, 0xFD, 0xF0, 0xEF, 0xEA, 0xF1, 0xFB, 0xFE, 0x04, 0x00, 0x03, 0x04, 0x05, 0x00, 0xFD, 0x01, 0x03, 0xFC, 0x01, 0xFD, 0x00, 0x02, 0x02, 0x02, 0x0D, 0x10, 0x10, 0x15, 0x0A, 0x01, 0xF9, 0xF7, 0xF9, 0xFD, 0xFF, 0x01, 0x04, 0xFE, 0x00, 0x02, 0x00, 0x01, 0x00, 0x02, 0x00, 0xFE, 0x03, 0x00, 0xFF, 0xFC, 0x00, 0x00, 0x08, 0x0B, 0x09, 0x0E, 0x0C, 0x0D, 0x06, 0x00, 0x03, 0x01, 0x00, 0x04, 0x01, 0x03, 0x00, 0x04, 0x02, 0x01, 0xFB, 0x03, 0xFD, 0x04, 0x00, 0xFC, 0x01, 0xFE, 0xFC, 0x00, 0x05, 0x0A, 0x08, 0x11, 0x11, 0x0F, 0x0A, 0x03, 0x02, 0x04, 0x00, 0xFF, 0xFF, 0xFB, 0xFD, 0xFE, 0xFE, 0xFD, 0x00, 0x00, 0x01, 0x00, 0xFE, 0xFF, 0x00, 0xFD, 0xFF, 0x00, 0x05, 0x06, 0x0C, 0x0C, 0x0D, 0x14, 0x0E, 0x0F, 0x0A, 0x06, 0x01, 0x03, 0xFF, 0xFE, 0x00, 0xF9, 0xFF, 0x02, 0x02, 0xFD, 0x01, 0xFE, 0x00, 0x00, 0xFD, 0xFC, 0xFD, 0x01, 0x02, 0xFF, 0x02, 0x07, 0x07, 0x10, 0x12, 0x0D, 0x04, 0x04, 0x00, 0x00, 0xF9, 0xF7, 0xFE, 0xFE, 0xFE, 0xFE, 0xFE, 0x03, 0x00, 0x03, 0x04, 0x00, 0x00, 0x00, 0x02, 0xFC, 0x00, 0xFE, 0x00, 0xFE, 0xFC, 0xFD, 0x00, 0xFC, 0xFB, 0xFB, 0xF9, 0xFD, 0xFF, 0xFC, 0x01, 0xFD, 0xFD, 0xFE, 0xFD, 0xFD, 0xFC, 0xFC, 0x01, 0x02, 0x04, 0x03, 0x00, 0x00, 0xFD, 0x00, 0xFA, 0xFF, 0xFC, 0xFD, 0xFD, 0xF9, 0xFF, 0xFE, 0xFF, 0xFD, 0x00, 0x01, 0x00, 0x02, 0xFF, 0x02, 0x01, 0x02, 0x04, 0x01, 0xFF, 0x00, 0x01, 0x00, 0xFF, 0x03, 0x02, 0xFD, 0xFF, 0xFF, 0xFC, 0xFF, 0xFD, 0xFC, 0xFD, 0xFD, 0x02, 0x00, 0x03, 0x00, 0x00, 0x01, 0x00, 0xFD, 0xFE, 0xFF, 0x00, 0x01, 0x01, 0x03, 0x00, 0xFF, 0xFD, 0xFD, 0xFE, 0x01, 0x02, 0x03, 0xFF, 0x00, 0xFF, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0xFF, 0xFE, 0x01, 0x00, 0xFE, 0x01, 0x00, 0x00, 0xFE]\n",
    "w1 = [0x00, 0xFD, 0x01, 0x00, 0x02, 0x00, 0x00, 0x01, 0x03, 0xFE, 0x02, 0x01, 0xFD, 0x01, 0xFE, 0xFF, 0xFE, 0xFD, 0x03, 0xFC, 0x02, 0xFC, 0x00, 0x00, 0x03, 0xFF, 0x01, 0x00, 0x02, 0x00, 0x01, 0xFE, 0xFD, 0x04, 0xFD, 0x00, 0x00, 0x04, 0xFF, 0xFD, 0xFF, 0x02, 0x01, 0x03, 0x00, 0x02, 0xFC, 0x01, 0x02, 0x02, 0xFE, 0xFF, 0x01, 0xFD, 0xFD, 0xFC, 0x00, 0x02, 0x01, 0x00, 0x01, 0xFE, 0x00, 0xFF, 0xFE, 0x03, 0xFD, 0x00, 0xFC, 0x00, 0xFE, 0x01, 0xFE, 0x02, 0xFC, 0x02, 0xFD, 0x02, 0xFE, 0x00, 0x00, 0x02, 0x01, 0xFD, 0x00, 0xFD, 0x03, 0x00, 0xFF, 0xFE, 0x01, 0xFE, 0x01, 0x01, 0x00, 0xFF, 0xFA, 0x01, 0xFD, 0xFE, 0xFE, 0x00, 0x00, 0xFF, 0x00, 0xFB, 0x00, 0xFC, 0x01, 0xFD, 0xFF, 0x03, 0x04, 0xFE, 0xFE, 0x00, 0x01, 0x00, 0xFF, 0xFC, 0x00, 0x01, 0xFB, 0x00, 0x03, 0x08, 0x04, 0x03, 0x04, 0x02, 0x05, 0x03, 0x08, 0x08, 0x00, 0x01, 0xFF, 0x02, 0xFE, 0x00, 0x02, 0xFC, 0xFD, 0xFE, 0x02, 0x02, 0xFC, 0xFF, 0xF9, 0xF8, 0xFE, 0xFF, 0x00, 0x07, 0x07, 0x04, 0xFE, 0xFC, 0x04, 0x07, 0x0C, 0x07, 0x07, 0x04, 0x02, 0x00, 0x04, 0x00, 0x00, 0x04, 0x04, 0xFF, 0x03, 0x00, 0xFE, 0x00, 0xF6, 0xF8, 0xF9, 0xF4, 0xFA, 0xFD, 0x00, 0xF7, 0xF5, 0xF9, 0x01, 0x07, 0x0A, 0x06, 0xFF, 0xFC, 0xFF, 0x03, 0x00, 0x00, 0xFD, 0xFE, 0xFD, 0x03, 0xFF, 0xFB, 0xFD, 0xFA, 0xF5, 0xF3, 0xF2, 0xED, 0xF3, 0xFC, 0xF9, 0xF5, 0xF9, 0xF7, 0xFB, 0x00, 0xFE, 0x01, 0xFB, 0xF9, 0xF9, 0x00, 0x00, 0x00, 0xFF, 0xFE, 0x00, 0x03, 0xFD, 0xFF, 0xF8, 0xF6, 0xF8, 0xF0, 0xF2, 0xED, 0xF2, 0xFB, 0x06, 0x00, 0x00, 0xFF, 0x00, 0xFF, 0xF8, 0xFA, 0xF4, 0xF7, 0xFE, 0xFF, 0x00, 0xFF, 0xFE, 0x02, 0x00, 0x00, 0x00, 0x01, 0xFA, 0xF8, 0xF8, 0xF4, 0xEC, 0xEA, 0xF6, 0x03, 0x0F, 0x15, 0x05, 0x02, 0xF9, 0xFA, 0xF7, 0xF5, 0xFA, 0xF6, 0xF8, 0xFA, 0x02, 0x01, 0xFD, 0x02, 0x04, 0xFC, 0x02, 0xFF, 0xFD, 0xFE, 0xFB, 0xF4, 0xF3, 0xF0, 0xF5, 0x02, 0x1D, 0x1D, 0x10, 0x00, 0xF8, 0xF1, 0xF0, 0xF9, 0xFB, 0xFE, 0xFC, 0xFF, 0xFF, 0x03, 0xFF, 0x02, 0x03, 0xFC, 0x00, 0xFE, 0xFB, 0xFD, 0xF7, 0xF6, 0xEB, 0xED, 0xEE, 0x08, 0x29, 0x2D, 0x10, 0xF8, 0xF6, 0xF0, 0xF6, 0xF5, 0xFB, 0x00, 0xFB, 0xFC, 0x00, 0x00, 0x00, 0x03, 0xFE, 0xFE, 0xFD, 0xFE, 0xFC, 0xFE, 0xF7, 0xF3, 0xE9, 0xEB, 0xF2, 0x0B, 0x32, 0x26, 0x09, 0xF2, 0xF0, 0xF8, 0xF4, 0xFF, 0xFE, 0x01, 0xFD, 0x02, 0x00, 0x00, 0x03, 0xFC, 0xFD, 0x04, 0xFE, 0xFC, 0xFD, 0xF9, 0xF7, 0xEE, 0xEA, 0xE0, 0xEE, 0x1A, 0x36, 0x24, 0xFF, 0xED, 0xEE, 0xF2, 0xF6, 0xF9, 0x01, 0xFE, 0x00, 0x02, 0x00, 0xFE, 0xFD, 0xFF, 0x01, 0xFD, 0x00, 0x00, 0x00, 0xFC, 0xF7, 0xF3, 0xEA, 0xE5, 0xF2, 0x1C, 0x30, 0x1A, 0xF4, 0xE8, 0xEF, 0xF7, 0xF6, 0xF7, 0xFB, 0xFC, 0x01, 0xFD, 0x00, 0x00, 0xFE, 0x02, 0xFF, 0x00, 0xFD, 0xFE, 0xFD, 0xFA, 0xF4, 0xF1, 0xE8, 0xEA, 0xFD, 0x23, 0x29, 0x13, 0xEA, 0xE4, 0xED, 0xF4, 0xF7, 0xFD, 0xFA, 0xFF, 0xFF, 0xFC, 0xFF, 0x01, 0x00, 0x03, 0x03, 0xFE, 0xFE, 0xFD, 0xF8, 0xF7, 0xF1, 0xF3, 0xF3, 0xF8, 0x05, 0x21, 0x25, 0x05, 0xEA, 0xE7, 0xF1, 0xF4, 0xF8, 0xF9, 0xFE, 0xFF, 0xFB, 0xFD, 0x00, 0x01, 0x02, 0xFE, 0xFC, 0x00, 0xFF, 0xFE, 0xF8, 0xFA, 0xF5, 0xF7, 0xF6, 0x02, 0x10, 0x20, 0x1E, 0xFB, 0xF0, 0xE9, 0xEE, 0xF5, 0xF7, 0xFC, 0xFF, 0xFA, 0x00, 0xFF, 0xFF, 0x00, 0x03, 0xFE, 0xFD, 0x02, 0xFD, 0xFD, 0xF6, 0xF6, 0xF9, 0xF7, 0xFD, 0x09, 0x11, 0x18, 0x0D, 0x01, 0xF0, 0xEC, 0xEE, 0xF3, 0xFA, 0xFA, 0xFF, 0xFD, 0xFE, 0x01, 0x00, 0x00, 0x01, 0x03, 0x01, 0x02, 0x01, 0xFC, 0xF8, 0xF7, 0xF8, 0x00, 0x06, 0x09, 0x0A, 0x11, 0x05, 0x01, 0xF7, 0xF4, 0xF3, 0xF3, 0xFC, 0xF9, 0xFD, 0x00, 0xFB, 0x00, 0xFF, 0x03, 0xFE, 0x02, 0x02, 0x01, 0x02, 0xFD, 0xF6, 0xFD, 0xFD, 0x01, 0x00, 0x02, 0x00, 0x06, 0x05, 0x01, 0xFD, 0xF4, 0xF5, 0xF6, 0xF8, 0xFF, 0xFE, 0x01, 0xFF, 0xFC, 0xFF, 0x03, 0x02, 0x01, 0xFF, 0xFC, 0xFD, 0x01, 0xFE, 0x02, 0x03, 0x01, 0x04, 0xFB, 0xF8, 0xF8, 0xFB, 0x01, 0x00, 0xFA, 0xFD, 0xF6, 0xF7, 0xFA, 0xFC, 0xFE, 0xFF, 0x01, 0xFC, 0x00, 0x00, 0x01, 0x04, 0x02, 0xFE, 0xFF, 0x03, 0x03, 0x08, 0x04, 0x02, 0xF8, 0xF1, 0xF7, 0x00, 0x04, 0x08, 0xFE, 0x00, 0xFA, 0xF9, 0xFD, 0x00, 0x02, 0x00, 0x00, 0x00, 0xFF, 0x00, 0x00, 0x03, 0x02, 0xFD, 0x02, 0x02, 0x06, 0x03, 0x05, 0xFA, 0xFB, 0xF9, 0xF6, 0xFD, 0x06, 0x05, 0x04, 0xFC, 0x00, 0xFB, 0xFD, 0xFD, 0x01, 0x00, 0xFC, 0xFE, 0xFE, 0xFC, 0x00, 0x01, 0x00, 0xFF, 0x00, 0xFE, 0xFE, 0xFE, 0x00, 0xF6, 0xF7, 0xF3, 0xF5, 0xF5, 0x00, 0x01, 0xFF, 0x01, 0x02, 0x01, 0x01, 0xFC, 0xFC, 0xFD, 0x01, 0xFD, 0xFD, 0xFE, 0xFC, 0x00, 0x00, 0xFE, 0x02, 0xFE, 0xFD, 0x00, 0x00, 0xFA, 0xF7, 0xF8, 0xFB, 0xFE, 0xFE, 0xFC, 0xFB, 0xFC, 0xFF, 0x00, 0xFE, 0xFD, 0x00, 0xFD, 0xFD, 0xFD, 0xFF, 0x04, 0x01, 0x00, 0x00, 0x01, 0x00, 0xFD, 0xFD, 0xFD, 0x00, 0xFB, 0xFE, 0xFD, 0x02, 0x02, 0x01, 0x00, 0xFF, 0x00, 0xFD, 0xFE, 0x00, 0xFF, 0xFC, 0x01, 0x03, 0xFE, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 0xFE, 0xFF, 0x02, 0x01, 0x00, 0x00, 0xFF, 0xFF, 0xFE, 0x00, 0x01, 0x03, 0xFF, 0x00, 0x02, 0x01, 0x00, 0x01, 0x00, 0xFD, 0x00, 0x00, 0x04]\n",
    "\n",
    "neuron_idx = 1\n",
    "if neuron_idx == 0 : \n",
    "    w_hex = w0\n",
    "else :\n",
    "    w_hex = w1\n",
    "\n",
    "\n",
    "saved_input_coe = [0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x18, 0x59, 0x7F, 0x09, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x51, 0x7E, 0x7C, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x50, 0x7E, 0x7B, 0x26, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x2F, 0x7E, 0x7E, 0x47, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x1E, 0x75, 0x7E, 0x79, 0x0D, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x67, 0x7E, 0x78, 0x26, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x50, 0x7E, 0x77, 0x27, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x2B, 0x7E, 0x7E, 0x4A, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x40, 0x75, 0x7E, 0x56, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x22, 0x75, 0x7E, 0x76, 0x15, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x11, 0x75, 0x7E, 0x76, 0x25, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x18, 0x53, 0x7E, 0x64, 0x25, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x5F, 0x7E, 0x76, 0x19, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x54, 0x7E, 0x77, 0x25, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x23, 0x7E, 0x7E, 0x52, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x5F, 0x7E, 0x56, 0x05, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x65, 0x7E, 0x66, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x7E, 0x7E, 0x6C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x7E, 0x7E, 0x7E, 0x79, 0x44, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x66, 0x75, 0x36, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00]\n",
    "saved_input_sample_flatten = saved_input_sample.view(-1).cpu().numpy().tolist()\n",
    "print(int8_convert_ratio, result_convert_ratio)\n",
    "w_int = (model_1.fc.weight.detach().cpu()[neuron_idx]*int8_convert_ratio).to(torch.int8)\n",
    "w_float = model_1.fc.weight.detach().cpu()[neuron_idx]\n",
    "res = 0\n",
    "res_int = 0\n",
    "res_float = 0.0\n",
    "for i, w in enumerate(w_hex) : \n",
    "    if w > 127 :\n",
    "        w = w - 256\n",
    "    res = res + w*saved_input_coe[i]\n",
    "    res_int = res_int + int(w_int[i])*saved_input_coe[i]\n",
    "    res_float = res_float + float(w_float[i])*saved_input_sample_flatten[i]\n",
    "\n",
    "hex_res = int_to_vhdl_hex(res,16)\n",
    "hex_res_int = int_to_vhdl_hex(res_int,16)\n",
    "hex_res_float = int_to_vhdl_hex(int(res_float*int8_convert_ratio),16)\n",
    "print(f\"hexa : {res}, {hex_res}, {res/result_convert_ratio:.4f}\")\n",
    "print(f\"int  : {res_int}, {hex_res_int}, {res_int/result_convert_ratio:.4f}\")\n",
    "print(f\"float: {res_float}, {(int(res_float*result_convert_ratio))}, {hex_res_float}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b007e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | test_loss=2.2990 | test_acc=12.96%\n",
      "Epoch 01 | test_loss=0.2253 | test_acc=93.49%\n",
      "Two-layer shapes: \n",
      "  W2_1 (128, 784) b2_1 (128,) \n",
      "  W2_2 (10, 128) b2_2 (10,)\n"
     ]
    }
   ],
   "source": [
    "# ===== Block 4: Two-layer MLP (128 -> 10) =====\n",
    "\n",
    "class TwoLayerMNIST(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden)  # 784 -> 128\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, 10)     # 128 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "model_2 = TwoLayerMNIST(hidden=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 1\n",
    "test_loss, test_acc = eval_multiclass(model_2, test_loader)\n",
    "print(f\"Epoch 0 | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model_2.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_2(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_loss, test_acc = eval_multiclass(model_2, test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | test_loss={test_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Export weights (float)\n",
    "W2_1 = model_2.fc1.weight.detach().cpu().numpy()  # (128,784)\n",
    "b2_1 = model_2.fc1.bias.detach().cpu().numpy()    # (128,)\n",
    "W2_2 = model_2.fc2.weight.detach().cpu().numpy()  # (10,128)\n",
    "b2_2 = model_2.fc2.bias.detach().cpu().numpy()    # (10,)\n",
    "\n",
    "print(\"Two-layer shapes:\",\n",
    "      \"\\n  W2_1\", W2_1.shape, \"b2_1\", b2_1.shape,\n",
    "      \"\\n  W2_2\", W2_2.shape, \"b2_2\", b2_2.shape)\n",
    "\n",
    "np.savez(\"mnist_l1_weights.npz\", W=W2_1, b_bin=b2_1)\n",
    "np.savez(\"mnist_l2_weights.npz\", W=W2_2, b_bin=b2_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
